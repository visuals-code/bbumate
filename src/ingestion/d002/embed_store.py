import os
import argparse
from pathlib import Path
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import logging
from tqdm import tqdm

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from langchain_community.vectorstores import Chroma


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# .env ë¡œë“œ
load_dotenv()

# ì²­í‚¹ íŒŒë¼ë¯¸í„° ìƒìˆ˜
DEFAULT_CHUNK_SIZE = 1000
DEFAULT_CHUNK_OVERLAP = 100


def extract_text_from_html(html_content: str) -> str:
    """HTMLì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ.

    - ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±° í›„ í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ ì„ë² ë”© í’ˆì§ˆì„ ë†’ì¸ë‹¤.
    - íƒœê·¸ë¥¼ ë³´ì¡´í•˜ì§€ ì•ŠìŒ: ë²¡í„°í™” ëŒ€ìƒì€ ì˜ë¯¸ í…ìŠ¤íŠ¸ ì¤‘ì‹¬.
    """
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        # script, style íƒœê·¸ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()

        text = soup.get_text(separator="\n", strip=True)
        return text

    except Exception as e:
        logger.warning(f"HTML íŒŒì‹± ì‹¤íŒ¨, raw text ì‚¬ìš©: {e}")
        return html_content


def load_html_documents(input_dir: Path) -> list[Document]:
    """HTML íŒŒì¼ë“¤ì„ Document ê°ì²´ë¡œ ë¡œë“œ.

    - íŒŒì¼ ë‹¨ìœ„ ì‹¤íŒ¨ëŠ” ì „ì²´ ë°°ì¹˜ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰í•œë‹¤.
    - ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ ê²½ë¡œ/ê¸¸ì´ë¥¼ ì €ì¥í•´ ì¶”ì ì„±ê³¼ ë””ë²„ê¹…ì„ ë•ëŠ”ë‹¤.
    """

    if not input_dir.exists():
        raise FileNotFoundError(f"ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")

    html_files = list(input_dir.glob("*.html"))
    if not html_files:
        raise ValueError(f"HTML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")

    documents = []
    failed_files = []

    logger.info(f"{len(html_files)}ê°œ HTML íŒŒì¼ ë°œê²¬")
    for file_path in tqdm(html_files, desc="HTML ë¡œë”©"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                html_content = f.read()

            # HTML íƒœê·¸ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            text = extract_text_from_html(html_content)

            if not text.strip():
                logger.warning(f"ë¹ˆ ë¬¸ì„œ ê±´ë„ˆëœ€: {file_path.name}")
                continue

            documents.append(
                Document(
                    page_content=text,
                    metadata={
                        "source": file_path.name,
                        "file_path": str(file_path),
                        "total_chars": len(text),
                    },
                )
            )

        except Exception as e:
            logger.error(f"{file_path.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            failed_files.append(file_path.name)

    if failed_files:
        logger.warning(
            f"ì‹¤íŒ¨í•œ íŒŒì¼ ({len(failed_files)}ê°œ): {', '.join(failed_files[:5])}"
            + (f" ì™¸ {len(failed_files)-5}ê°œ" if len(failed_files) > 5 else "")
        )

    return documents


def embed_from_html(
    input_dir: Path,
    persist_dir: Path,
    collection_name: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
):
    """HTML íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ì—¬ VectorDBì— ì €ì¥.

    - í…ìŠ¤íŠ¸ ì •ì œ â†’ ë¶„í•  â†’ ì„ë² ë”© â†’ VectorDB ì €ì¥ ìˆœì„œë¡œ ì²˜ë¦¬í•œë‹¤.
    - ì§„í–‰ë¥ /ìš”ì•½ ë¡œê¹…ì„ í†µí•´ ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œ ê°€ì‹œì„±ì„ í™•ë³´í•œë‹¤.
    """

    # HTML ë¬¸ì„œ ë¡œë“œ
    logger.info(f"HTML íŒŒì¼ ë¡œë“œ ì¤‘: {input_dir}")
    documents = load_html_documents(input_dir)
    logger.info(f"{len(documents)}ê°œ HTML íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

    if not documents:
        raise ValueError("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ë¶„í• 
    logger.info("í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    splits = splitter.split_documents(documents)

    # ì²­í¬ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    # - ê²€ìƒ‰ ê²°ê³¼ê°€ ì›ë¬¸ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì¸ì§€ ì¶”ì í•˜ê¸° ìœ„í•œ ìµœì†Œ ì •ë³´
    for i, split in enumerate(splits):
        split.metadata["chunk_index"] = i

    logger.info(
        f"ì´ {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ (chunk={chunk_size}, overlap={chunk_overlap})"
    )

    # ì˜ˆìƒ ë¹„ìš© ì•ˆë‚´ (ì„ íƒì‚¬í•­)
    total_chars = sum(len(doc.page_content) for doc in splits)
    logger.info(f"ì´ {total_chars:,}ì ì„ë² ë”© ì˜ˆì • (ì•½ {len(splits)}íšŒ API í˜¸ì¶œ)")

    # Upstage ì„ë² ë”© ì´ˆê¸°í™”
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("í™˜ê²½ë³€ìˆ˜ UPSTAGE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤ (.env í™•ì¸).")

    try:
        # ì„ë² ë”© ëª¨ë¸ì€ í™˜ê²½ë³€ìˆ˜ë¡œ êµì²´ ê°€ëŠ¥. ê¸°ë³¸ê°’ì€ Upstage í‘œì¤€ ì„ë² ë”© ëª¨ë¸.
        embedding_model = os.getenv("UPSTAGE_EMBEDDING_MODEL", "embedding-query")
        embeddings = UpstageEmbeddings(api_key=api_key, model=embedding_model)
        logger.info(f"ì„ë² ë”© ëª¨ë¸: {embedding_model}")

    except Exception as e:
        raise ValueError(f"Upstage ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # Chroma VectorDB ì €ì¥
    persist_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info("ì„ë² ë”© ë° VectorDB ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
        # - ëŒ€ìš©ëŸ‰(ìˆ˜ì²œ ì²­í¬)ì—ì„œ í•œ ë²ˆì— from_documentsë¥¼ í˜¸ì¶œí•˜ë©´ ë©”ëª¨ë¦¬/ì‹œê°„ ê¸‰ì¦
        # - 1íšŒ ìƒì„± í›„ add_documentsë¡œ ì¦ë¶„ ì¶”ê°€í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        batch_size = 100
        if len(splits) > batch_size:
            logger.info(f"{batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬")
            vectordb = None
            for i in tqdm(range(0, len(splits), batch_size), desc="ì„ë² ë”©"):
                batch = splits[i : i + batch_size]
                if vectordb is None:
                    vectordb = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        persist_directory=str(persist_dir),
                        collection_name=collection_name,
                    )
                else:
                    vectordb.add_documents(batch)
        else:
            vectordb = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory=str(persist_dir),
                collection_name=collection_name,
            )

        # from_documentsëŠ” ìƒì„± ì‹œ ì €ì¥ë˜ë©°, add_documentsëŠ” ìë™ ë°˜ì˜ë¨

    except Exception as e:
        raise RuntimeError(f"VectorDB ìƒì„± ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ í™•ì¸
    try:
        # ì‹¤ì œ ë¬¸ì„œì˜ ë‹¨ì–´ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        # - "test" ê°™ì€ ë¬´ì˜ë¯¸ ì¿¼ë¦¬ ëŒ€ì‹ , ì²« ì²­í¬ì˜ ì‹¤ì œ í† í°ìœ¼ë¡œ ì €ì¥ ê²€ì¦
        first_words = splits[0].page_content[:20].split()[0] if splits else "test"
        test_results = vectordb.similarity_search(first_words, k=1)
        doc_count = len(splits)

        logger.info("=" * 60)
        logger.info("ì„ë² ë”© ì™„ë£Œ ë° ì €ì¥ ì™„ë£Œ!")
        logger.info(f"VectorDB ìœ„ì¹˜: {persist_dir}")
        logger.info(f"ì»¬ë ‰ì…˜ ì´ë¦„: {collection_name}")
        logger.info(f"ì €ì¥ëœ ì²­í¬ ìˆ˜: {doc_count}")
        logger.info(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        if doc_count:
            logger.info(f"í‰ê·  ì²­í¬ í¬ê¸°: {total_chars // doc_count:,}ì")

        if test_results:
            logger.info(f"ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°:")
            logger.info(f"   â€¢ source: {test_results[0].metadata.get('source')}")
            logger.info(
                f"   â€¢ chunk_index: {test_results[0].metadata.get('chunk_index')}"
            )
            logger.info(
                f"   â€¢ total_chars: {test_results[0].metadata.get('total_chars')}"
            )
            logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì¿¼ë¦¬: '{first_words}')")
        logger.info("=" * 60)

    except Exception as e:
        logger.warning(f"ê²°ê³¼ í™•ì¸ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ (ì €ì¥ì€ ì™„ë£Œë¨): {e}")


def main():
    parser = argparse.ArgumentParser(
        description="HTML íŒŒì¼ë“¤ì„ ì„ë² ë”©í•˜ì—¬ Chroma VectorDBì— ì €ì¥"
    )
    parser.add_argument("--domain", required=True, help="ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: d002)")
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=DEFAULT_CHUNK_SIZE,
        help=f"ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: {DEFAULT_CHUNK_SIZE})",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=DEFAULT_CHUNK_OVERLAP,
        help=f"ì²­í¬ ì˜¤ë²„ë© (ê¸°ë³¸ê°’: {DEFAULT_CHUNK_OVERLAP})",
    )
    args = parser.parse_args()

    base_dir = Path("data") / args.domain
    input_dir = base_dir / "htmls"
    persist_dir = base_dir / "vector_store"
    collection_name = args.domain

    try:
        embed_from_html(
            input_dir=input_dir,
            persist_dir=persist_dir,
            collection_name=collection_name,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
        )
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        exit(1)


if __name__ == "__main__":
    main()
