import os
import time
from typing import List, Dict, Any

from langchain_chroma import Chroma
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv


def load_vector_db(domain: str = "d002") -> Chroma:
    """ë„ë©”ì¸ë³„ Chroma VectorDB ë¡œë“œ (Upstage ì„ë² ë”© ì¼ê´€í™”)."""
    # .env ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼)
    load_dotenv()
    persist_dir = f"data/{domain}/vector_store"

    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("UPSTAGE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    embedding_model = os.getenv("UPSTAGE_EMBEDDING_MODEL", "embedding-query")
    embeddings = UpstageEmbeddings(api_key=api_key, model=embedding_model)

    return Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
        collection_name=domain,
    )


def load_llm() -> ChatUpstage:
    # .env ë¡œë“œ (í•œ ë²ˆ ë” ë³´ì¥)
    load_dotenv()
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("UPSTAGE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    model = os.getenv("UPSTAGE_CHAT_MODEL", "solar-1-mini-chat")

    return ChatUpstage(api_key=api_key, model=model)


def _format_docs(docs: List[Any]) -> str:
    lines = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "unknown")
        content = (d.page_content or "").strip()

        if len(content) > 2000:
            content = content[:2000] + "..."

        lines.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source}\n{content}")

    return "\n\n---\n\n".join(lines) if lines else "ì œê³µëœ ë¬¸ì„œ ì—†ìŒ"


def build_rag_chain(domain: str = "d002"):
    vectordb = load_vector_db(domain)
    llm = load_llm()

    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
         ë‹¹ì‹ ì€ ì‹ í˜¼ë¶€ë¶€ ì§€ì›ì •ì±… ë„ë©”ì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
         ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ë‹µë³€í•˜ì§€ ë§ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
         ë‹µë³€ ëì— ì°¸ê³ í•œ ì¶œì²˜ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.
         ì»¨í…ìŠ¤íŠ¸:\n{context}
         """.strip(),
            ),
            ("human", "ì§ˆë¬¸: {question}"),
        ]
    )

    chain = (
        {
            "context": retriever | _format_docs,
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain, retriever


def run_rag(query: str, domain: str = "d002", verbose: bool = False) -> Dict[str, Any]:
    start = time.perf_counter()
    chain, retriever = build_rag_chain(domain)
    answer = chain.invoke(query)
    # ì¶œì²˜ ìˆ˜ì§‘: ë™ì¼ retrieverë¡œ ë‹¤ì‹œ í˜¸ì¶œí•˜ì—¬ ì†ŒìŠ¤ í™•ë³´
    docs = retriever.invoke(query)
    sources = list({d.metadata.get("source", "unknown") for d in docs})
    duration_ms = int((time.perf_counter() - start) * 1000)

    if verbose:
        print("ğŸ§© [ì§ˆë¬¸]", query)
        print("â±ï¸  [ì†Œìš”(ms)]", duration_ms)
        print("ğŸ’¬ [ë‹µë³€]", answer)
        print("ğŸ“š [ì¶œì²˜]", sources)

    return {"answer": answer, "sources": sources, "duration_ms": duration_ms}


## ì‹¤í–‰ í…ŒìŠ¤íŠ¸
# python -c "from src.chains.rag_chain_d002 import run_rag; res = run_rag('ì‹ í˜¼ë¶€ë¶€ ì „ì„¸ìê¸ˆëŒ€ì¶œ ì¡°ê±´ ì•Œë ¤ì¤˜','d002'); print(res['answer']); print(res['sources']); print(str(res['duration_ms']) + ' ms')"
