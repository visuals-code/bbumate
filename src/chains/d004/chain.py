# -*- coding: utf-8 -*-
import os
import sys
import re
import time
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

sys.path.insert(0, "/Users/a/KDT_BE13_Toy_Project4/src/chains/d004")
sys.path.insert(0, "/Users/a/KDT_BE13_Toy_Project4/src/generation/d004")
sys.path.insert(0, "/Users/a/KDT_BE13_Toy_Project4/src/ingestion/d004")
sys.path.insert(0, "/Users/a/KDT_BE13_Toy_Project4/src/retrieval/d004")
sys.path.insert(0, "/Users/a/KDT_BE13_Toy_Project4/src/test/d004")

from query_router import QueryRouter
from retrieval import load_retriever, format_docs
from grader import DocumentGrader
from query_rewriter import QueryRewriter
from web_search_fallback import WebSearchFallback


class AdvancedRAGChain:
    def __init__(
        self,
        api_key: str = None,
        model: str = None,
        db_path: str = None,
        max_rewrite_attempts: int = 2,
    ):
        load_dotenv()

        self.api_key = api_key or os.getenv("UPSTAGE_API_KEY")
        self.model = model or os.getenv("UPSTAGE_CHAT_MODEL", "solar-1-mini-chat")
        self.db_path = db_path or os.getenv("CHROMA_DB_DIR", "./chroma_storage")
        self.collection_name = os.getenv("COLLECTION_NAME", "pdf_promotion_chunks")
        self.max_rewrite_attempts = max_rewrite_attempts

        # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = QueryRouter(api_key=self.api_key, model=self.model)
        self.grader = DocumentGrader(api_key=self.api_key, model=self.model)
        self.rewriter = QueryRewriter(api_key=self.api_key, model=self.model)
        self.web_search = WebSearchFallback(api_key=self.api_key, model=self.model)

        # Retriever ì´ˆê¸°í™”
        self.retriever = load_retriever(
            db_path=self.db_path, collection_name=self.collection_name, k=3
        )

        # LLM ì´ˆê¸°í™”
        self.llm = ChatUpstage(api_key=self.api_key, model=self.model)

        # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.answer_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
                    "ì‹ í˜¼ë¶€ë¶€ ì •ì±…ì´ë‚˜ í˜œíƒê³¼ ê´€ë ¨ë˜ì§€ ì•Šìœ¼ë©´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.\n"
                    "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.\n\n"
                    "ì»¨í…ìŠ¤íŠ¸:\n{context}",
                ),
                ("human", "ì§ˆë¬¸: {question}"),
            ]
        )

        self.answer_chain = self.answer_prompt | self.llm | StrOutputParser()

    def _extract_sources(self, documents: List[Document]) -> List[Dict]:
        sources = []
        seen_sources = set()

        for doc in documents:
            metadata = doc.metadata
            source_file = metadata.get("source_file", metadata.get("source", ""))

            # ì¤‘ë³µ ì²´í¬
            source_key = source_file
            if source_key in seen_sources:
                continue
            seen_sources.add(source_key)

            # title ì¶”ì¶œ
            title = metadata.get("heading", "")
            if not title:
                title = Path(source_file).stem.replace("_", " ").title()

            # url ì¶”ì¶œ ë° ë””ë²„ê¹…
            url = metadata.get("url", None)

            # ğŸ” ë””ë²„ê¹… ì¶œë ¥ (ë‚˜ì¤‘ì— ì œê±° ê°€ëŠ¥)
            print(f"[DEBUG] ì¶œì²˜: {Path(source_file).name}")
            print(f"  - URL: {url}")
            print(f"  - ì „ì²´ ë©”íƒ€ë°ì´í„°: {metadata}")

            sources.append({"title": title, "url": url, "source": source_file})

        return sources

    def _extract_web_sources(self, web_result: Dict) -> List[Dict]:
        sources = []
        for result in web_result.get("results", []):
            sources.append(
                {"title": result["title"], "url": result["url"], "source": "web_search"}
            )
        return sources

    def _format_markdown(self, text: str) -> str:
        # ê¸ˆì•¡ íŒ¨í„´: ìˆ«ì
        # ë¹„ìœ¨ íŒ¨í„´: ìˆ«ì + %
        pattern = r"(\d+(?:,\d+)*(?:\.\d+)?(?:ë§Œì›|ì–µì›|ì²œì›|ì›|%))"
        text = re.sub(pattern, r"**\1**", text)

        # ë¬¸ì¥ ë‹¨ìœ„ ì¤„ë°”ê¿ˆ
        text = re.sub(r"\.\s+", ".\\n\\n", text)

        return text.strip()

    def _format_html(self, text: str) -> str:

        # ê¸ˆì•¡/ë¹„ìœ¨ íŒ¨í„´
        pattern = r"(\d+(?:,\d+)*(?:\.\d+)?(?:ë§Œì›|ì–µì›|ì²œì›|ì›|%))"
        text = re.sub(pattern, r"<strong>\1</strong>", text)

        # ë¬¸ì¥ ë‹¨ìœ„ ì¤„ë°”ê¿ˆ
        text = re.sub(r"\.\s+", ".<br/><br/>", text)

        # divë¡œ ê°ì‹¸ê¸°
        return f"<div>{text}</div>"

    def invoke(
        self, question: str, region: str = None, housing_type: str = None
    ) -> Dict:

        print(f"\n{'='*60}")
        print(f"[RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘] ì§ˆë¬¸: {question}")
        if region:
            print(f"  - ê±°ì£¼ì§€ì—­: {region}")
        if housing_type:
            print(f"  - ì£¼ê±°í˜•íƒœ: {housing_type}")
        print(f"{'='*60}")

        # Step 1: Query Router - ì§ˆë¬¸ ëª…í™•ì„± íŒë‹¨
        print("\n Query Router: ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
        routing_result = self.router.route(question)

        if routing_result["status"] == "WEB_SEARCH":
            print(f"  â†’ ì‹¤ì‹œê°„ ì •ë³´ í•„ìš”. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìš°íšŒí•©ë‹ˆë‹¤.")
            web_result = self.web_search.search_and_answer(question)
            answer_text = web_result["answer"]
            web_sources = self._extract_web_sources(web_result)

            return {
                "answer": answer_text,
                "answer_md": self._format_markdown(answer_text),
                "answer_html": self._format_html(answer_text),
                "sources": web_sources,
                "original_question": question,
                "final_question": question,
                "routing_status": "WEB_SEARCH",
                "documents_retrieved": 0,
                "relevant_documents": 0,
                "source": "web_search",
                "rewrite_count": 0,
            }

        print(f"  â†’ ì§ˆë¬¸ì´ ëª…í™•í•©ë‹ˆë‹¤. ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # Step 2: Retrieval with potential rewriting
        current_question = question
        rewrite_count = 0
        relevant_docs = []
        documents = []

        # metadata í•„í„° êµ¬ì„± (Chroma í˜•ì‹)
        metadata_filter = None
        if region and housing_type:
            # ë‘ ì¡°ê±´ ëª¨ë‘ ìˆìœ¼ë©´ $and ì‚¬ìš©
            metadata_filter = {
                "$and": [
                    {"region": {"$eq": region}},
                    {"housing_type": {"$eq": housing_type}},
                ]
            }
        elif region:
            # regionë§Œ ìˆìœ¼ë©´
            metadata_filter = {"region": {"$eq": region}}
        elif housing_type:
            # housing_typeë§Œ ìˆìœ¼ë©´
            metadata_filter = {"housing_type": {"$eq": housing_type}}

        for attempt in range(self.max_rewrite_attempts + 1):
            if metadata_filter:
                print(f"  â†’ í•„í„° ì ìš©: {metadata_filter}")
                # vectorstoreì— ì§ì ‘ ì ‘ê·¼
                documents = self.retriever.vectorstore.similarity_search(
                    current_question, k=3, filter=metadata_filter
                )
            else:
                documents = self.retriever.invoke(current_question)

            # ë¬¸ì„œ ê²€ìƒ‰ (í•„í„° ì ìš©)
            start_time = time.time()
            if metadata_filter:
                # Chromaì˜ where í•„í„° ì‚¬ìš©
                documents = self.retriever.vectorstore.similarity_search(
                    current_question, k=3, filter=metadata_filter
                )
            else:
                documents = self.retriever.invoke(current_question)

            search_time = time.time() - start_time
            print(f"  â†’ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ (ì†Œìš”ì‹œê°„: {search_time:.2f}ì´ˆ)")

            if not documents:
                print("  â†’ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                if attempt < self.max_rewrite_attempts:
                    print(f"\n Query Rewriter: ì§ˆë¬¸ ì¬ì‘ì„± ì¤‘...")
                    current_question = self.rewriter.rewrite_with_history(
                        question, attempt
                    )
                    rewrite_count += 1
                    print(f"  â†’ ì¬ì‘ì„±ëœ ì§ˆë¬¸: {current_question}")
                    continue
                else:
                    break

            # Step 3: Grader - ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
            print(f"\n Grader: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì¤‘...")
            grading_result = self.grader.grade_documents(current_question, documents)

            print(
                f"  â†’ ê´€ë ¨ ë¬¸ì„œ: {len(grading_result['relevant_documents'])}/{grading_result['total_count']}"
            )

            relevant_docs = grading_result["relevant_documents"]

            if not relevant_docs and attempt == 0:
                print(f"  â†’ ì²« ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                print(f"\n Web Search: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
                web_result = self.web_search.search_and_answer(question)
                answer_text = web_result["answer"]
                web_sources = self._extract_web_sources(web_result)

                return {
                    "answer": answer_text,
                    "answer_md": self._format_markdown(answer_text),
                    "answer_html": self._format_html(answer_text),
                    "sources": web_sources,
                    "original_question": question,
                    "final_question": current_question,
                    "routing_status": "CLEAR",
                    "documents_retrieved": len(documents),
                    "relevant_documents": 0,
                    "source": "web_search",
                    "rewrite_count": 0,
                }

            # ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ë‹µë³€ ìƒì„±
            if relevant_docs:
                break

            # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ê³  ì¬ì‹œë„ ê°€ëŠ¥í•˜ë©´ ì§ˆë¬¸ ì¬ì‘ì„±
            if attempt < self.max_rewrite_attempts:
                print(f"\n Query Rewriter: ì§ˆë¬¸ ì¬ì‘ì„± ì¤‘...")
                current_question = self.rewriter.rewrite_with_history(question, attempt)
                rewrite_count += 1
                print(f"  â†’ ì¬ì‘ì„±ëœ ì§ˆë¬¸: {current_question}")
            else:
                print(f"  â†’ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")

        # Step 5: Web Search Fallback (ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°)
        if not relevant_docs:
            print(f"\n Web Search: ì›¹ ê²€ìƒ‰ í´ë°± ìˆ˜í–‰ ì¤‘...")
            web_result = self.web_search.search_and_answer(question)

            answer_text = web_result["answer"]

            web_sources = self._extract_web_sources(web_result)

            return {
                "answer": answer_text,
                "answer_md": self._format_markdown(answer_text),
                "answer_html": self._format_html(answer_text),
                "sources": web_sources,
                "original_question": question,
                "final_question": current_question,
                "routing_status": "CLEAR",
                "documents_retrieved": len(documents) if documents else 0,
                "relevant_documents": 0,
                "source": web_result["source"],
                "rewrite_count": rewrite_count,
            }

        # Step 6: LLM Answer Generation
        print(f"\n LLM Answer Generation: ë‹µë³€ ìƒì„± ì¤‘...")
        context = format_docs(relevant_docs)

        start_time = time.time()
        answer = self.answer_chain.invoke(
            {"context": context, "question": current_question}
        )
        generation_time = time.time() - start_time
        print(f"  â†’ ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {generation_time:.2f}ì´ˆ)")

        # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        sources = self._extract_sources(relevant_docs)

        print(f"\n{'='*60}")
        print(f"[RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ]")
        print(f"{'='*60}")

        return {
            "answer": answer,
            "answer_md": self._format_markdown(answer),
            "answer_html": self._format_html(answer),
            "sources": sources,
            "original_question": question,
            "final_question": current_question,
            "routing_status": "CLEAR",
            "documents_retrieved": len(documents) if documents else 0,
            "relevant_documents": len(relevant_docs),
            "source": "vectorstore",
            "rewrite_count": rewrite_count,
        }
