import os
import logging
from typing import List

from dotenv import load_dotenv
from langchain_upstage import ChatUpstage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document


logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)
load_dotenv()


# --- 1. Upstage LLM ì—°ê²° ---
def load_llm(temperature: float = 0.3) -> ChatUpstage:
    """Upstage Chat ëª¨ë¸ ì´ˆê¸°í™”"""
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("UPSTAGE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    # ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©
    model_name = os.getenv("UPSTAGE_CHAT_MODEL", "solar-mini")

    try:
        llm = ChatUpstage(api_key=api_key, model=model_name, temperature=temperature)
        logger.info(f"âœ… LLM ì´ˆê¸°í™”: {model_name} (temp={temperature})")
        return llm
    except Exception as e:
        raise ValueError(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# --- 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì¸ë´íŠ¸ ì œê±°) ---
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """ë‹¹ì‹ ì€ ì‹ í˜¼ë¶€ë¶€ ì§€ì›ì •ì±… ë„ë©”ì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë¥¼ ê²½ìš° "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
- ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
- ë°˜ë“œì‹œ ë‹µë³€ ëì— ì°¸ê³ í•œ ì¶œì²˜ë¥¼ ë‚˜ì—´í•˜ì„¸ìš” (ì˜ˆ: [ì¶œì²˜: íŒŒì¼ëª….pdf])

ì»¨í…ìŠ¤íŠ¸:
{context}""",
        ),
        ("human", "{question}"),
    ]
)

output_parser = StrOutputParser()


# --- 3. ë¬¸ì„œ í¬ë§·íŒ… ---
def _format_docs(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì´ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not docs:
        return "ì œê³µëœ ë¬¸ì„œ ì—†ìŒ"

    formatted = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", "unknown")
        content = (doc.page_content or "").strip()

        # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì˜ë¼ëƒ„
        if len(content) > 2000:
            content = content[:2000] + "..."

        formatted.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source}\n{content}")

    return "\n\n---\n\n".join(formatted)


# --- 4. ì‘ë‹µ ìƒì„± ---
def generate_response(
    user_query: str, retrieved_docs: List[Document], temperature: float = 0.3
) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""

    # ë¬¸ì„œ ì—†ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
    if not retrieved_docs:
        logger.warning("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."

    try:
        llm = load_llm(temperature=temperature)
        chain = prompt | llm | output_parser

        context = _format_docs(retrieved_docs)
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì")

        response = chain.invoke({"context": context, "question": user_query})

        return response.strip()

    except Exception as e:
        logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# --- 5. í†µí•© RAG í•¨ìˆ˜ (ì˜µì…˜) ---
def rag_pipeline(query: str, vectordb, k: int = 5, temperature: float = 0.3) -> dict:
    """ê²€ìƒ‰ + ìƒì„±ì„ í•œë²ˆì— ì²˜ë¦¬"""
    # ê²€ìƒ‰
    logger.info(f"ğŸ” ê²€ìƒ‰: '{query}'")
    docs = vectordb.similarity_search(query, k=k)

    if not docs:
        return {
            "query": query,
            "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
        }

    # ìƒì„±
    logger.info(f"ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
    answer = generate_response(query, docs, temperature=temperature)

    # ì¶œì²˜ ì •ë¦¬
    sources = [doc.metadata.get("source", "unknown") for doc in docs]

    return {
        "query": query,
        "answer": answer,
        "sources": list(set(sources)),  # ì¤‘ë³µ ì œê±°
        "num_docs": len(docs),
    }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ë¬¸ì„œ
    from langchain.schema import Document

    fake_docs = [
        Document(
            page_content="ë²„íŒ€ëª© ì „ì„¸ìê¸ˆëŒ€ì¶œ ê¸ˆë¦¬ëŠ” ì—° 1.8~2.4%ì…ë‹ˆë‹¤. "
            "ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° 0.2%p ìš°ëŒ€ê¸ˆë¦¬ê°€ ì ìš©ë©ë‹ˆë‹¤.",
            metadata={"source": "ì£¼íƒë„ì‹œê¸°ê¸ˆ_2024_ê³µê³ .pdf"},
        ),
        Document(
            page_content="ë””ë”¤ëŒëŒ€ì¶œ ì‹ í˜¼ë¶€ë¶€ íŠ¹ë¡€ëŠ” ì—° 2.15~3.0% ê¸ˆë¦¬ë¡œ "
            "ìµœëŒ€ 3.6ì–µì›ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
            metadata={"source": "êµ­í† êµí†µë¶€_ì£¼íƒê¸ˆìœµì•ˆë‚´.pdf"},
        ),
    ]

    query = "ì‹ í˜¼ë¶€ë¶€ ì „ì„¸ìê¸ˆëŒ€ì¶œ ê¸ˆë¦¬ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

    print("=" * 70)
    print(f"ì§ˆë¬¸: {query}")
    print("=" * 70)

    answer = generate_response(query, fake_docs)
    print(f"\në‹µë³€:\n{answer}\n")

    print("=" * 70)
